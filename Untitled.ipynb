{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#taken from https://github.com/firascherif/ABSA-BERT-pair/blob/master/generate/data_utils_sentihood.py\n",
    "\n",
    "# Reference: https://github.com/liufly/delayed-memory-update-entnet\n",
    "\n",
    "from __future__ import absolute_import\n",
    "\n",
    "import json\n",
    "import operator\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import xml.etree.ElementTree\n",
    "\n",
    "# import nltk\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def load_task(data_dir, aspect2idx):\n",
    "    in_file = os.path.join(data_dir, 'sentihood-train.json')\n",
    "    train = parse_sentihood_json(in_file)\n",
    "    in_file = os.path.join(data_dir, 'sentihood-dev.json')\n",
    "    dev = parse_sentihood_json(in_file)\n",
    "    in_file = os.path.join(data_dir, 'sentihood-test.json')\n",
    "    test = parse_sentihood_json(in_file)\n",
    "\n",
    "    train = convert_input(train, aspect2idx)\n",
    "    train_aspect_idx = get_aspect_idx(train, aspect2idx)\n",
    "    train = tokenize(train)\n",
    "    dev = convert_input(dev, aspect2idx)\n",
    "    dev_aspect_idx = get_aspect_idx(dev, aspect2idx)\n",
    "    dev = tokenize(dev)\n",
    "    test = convert_input(test, aspect2idx)\n",
    "    test_aspect_idx = get_aspect_idx(test, aspect2idx)\n",
    "    test = tokenize(test)\n",
    "\n",
    "    return (train, train_aspect_idx), (dev, dev_aspect_idx), (test, test_aspect_idx)\n",
    "\n",
    "\n",
    "# def get_aspect_idx(data, aspect2idx):\n",
    "#     ret = []\n",
    "#     for _, _, _, aspect, _ in data:\n",
    "#         ret.append(aspect2idx[aspect])\n",
    "#     assert len(data) == len(ret)\n",
    "#     return np.array(ret)\n",
    "\n",
    "\n",
    "def parse_sentihood_json(in_file):\n",
    "    with open(in_file) as f:\n",
    "        data = json.load(f)\n",
    "    ret = []\n",
    "    for d in data:\n",
    "        text = d['text']\n",
    "        sent_id = d['id']\n",
    "        opinions = []\n",
    "        targets = set()\n",
    "        for opinion in d['opinions']:\n",
    "            sentiment = opinion['sentiment']\n",
    "            aspect = opinion['aspect']\n",
    "            target_entity = opinion['target_entity']\n",
    "            targets.add(target_entity)\n",
    "            opinions.append((target_entity, aspect, sentiment))\n",
    "        ret.append((sent_id, text, opinions))\n",
    "    return ret\n",
    "\n",
    "\n",
    "def convert_input(data, all_aspects):\n",
    "    ret = []\n",
    "    for sent_id, text, opinions in data:\n",
    "        for target_entity, aspect, sentiment in opinions:\n",
    "            if aspect not in all_aspects:\n",
    "                continue\n",
    "            ret.append((sent_id, text, target_entity, aspect, sentiment))\n",
    "        assert 'LOCATION1' in text\n",
    "        targets = set(['LOCATION1'])\n",
    "        if 'LOCATION2' in text:\n",
    "            targets.add('LOCATION2')\n",
    "        for target in targets:\n",
    "            aspects = set([a for t, a, _ in opinions if t == target])\n",
    "            none_aspects = [a for a in all_aspects if a not in aspects]\n",
    "            for aspect in none_aspects:\n",
    "                ret.append((sent_id, text, target, aspect, 'None'))\n",
    "    return ret\n",
    "\n",
    "\n",
    "def tokenize(data):\n",
    "    ret = []\n",
    "    for sent_id, text, target_entity, aspect, sentiment in data:\n",
    "        new_text = nltk.word_tokenize(text)\n",
    "        new_aspect = aspect.split('-')\n",
    "        ret.append((sent_id, new_text, target_entity, new_aspect, sentiment))\n",
    "    return ret\n",
    "\n",
    "\n",
    "def all_aspects(data, aspect2idx):\n",
    "    aspect_logits = []\n",
    "    logit = np.zeros(12) #taking care of 0 % 12 \n",
    "    for i , (sent_id, text, target, aspect, sentiment) in enumerate(data, start = 1):\n",
    "        if i % 12 == 0:\n",
    "            aspect_logits.append(logit)\n",
    "            logit = np.zeros(12)\n",
    "        if sentiment != \"None\":\n",
    "            logit[aspect2idx[aspect]] = 1\n",
    "    return aspect_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev = parse_sentihood_json(\"sentihood-dev.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "aspect2idx = {\n",
    "    'general': 0,\n",
    "    'price': 1,\n",
    "    'transit-location': 2,\n",
    "    'safety': 3,\n",
    "    'live' : 4,\n",
    "    'quiet' : 5,\n",
    "    'dining' : 6,\n",
    "    'nightlife' : 7,\n",
    "    'touristy' : 8,\n",
    "    'shopping' : 9,\n",
    "    'green-culture' : 10,\n",
    "    'multicultural' : 11,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(322,\n",
       " '( LOCATION1 is the nearest tube station , just about a 4 min walk ) Youre lucky youre going to be studying in London',\n",
       " [])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "converted_dev = convert_input(dev, aspect2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(302,\n",
       " ' LOCATION1 is just a normal area that happens to have an alternative market',\n",
       " 'LOCATION1',\n",
       " 'shopping',\n",
       " 'Positive')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "converted_dev[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_aspect_id = get_aspect_idx(converted_dev, aspect2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.utils\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "# from torch.optim import lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, datasets, models\n",
    "from torchvision.utils import save_image\n",
    "import os\n",
    "\n",
    "from dataloader import SentihoodDataset\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "data_dir = os.path.join(current_dir, \"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "def pad_collate(batch):\n",
    "    batch_embedded_text, a, b, c, d = zip(*batch)\n",
    "    lens = [x.shape[0] for x in batch_embedded_text]\n",
    "    \n",
    "    batch_embedded_text = pad_sequence(batch_embedded_text, batch_first=True, padding_value=0)\n",
    "    \n",
    "    return batch_embedded_text, lens, a, b, c, d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_dataset = SentihoodDataset(\n",
    "        data_dir,\n",
    "        dataset_type='train',\n",
    "        transform=None,\n",
    "        condition_on_number = False\n",
    "    )\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=10, shuffle=True, num_workers=0, collate_fn = pad_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_text, lens, target_index, aspect_logit, c_aspect, sentiment_one_hot = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 39, 768])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedded_text.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[31, 39, 17, 23, 4, 22, 16, 22, 30, 32]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_index[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aspect_logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 1.])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 25, 300])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([25, 300])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.squeeze(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = [[1,2,3], [4,5]]\n",
    "l = [[1], [2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
