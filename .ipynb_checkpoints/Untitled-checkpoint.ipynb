{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#taken from https://github.com/firascherif/ABSA-BERT-pair/blob/master/generate/data_utils_sentihood.py\n",
    "\n",
    "# Reference: https://github.com/liufly/delayed-memory-update-entnet\n",
    "\n",
    "from __future__ import absolute_import\n",
    "\n",
    "import json\n",
    "import operator\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import xml.etree.ElementTree\n",
    "\n",
    "# import nltk\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def load_task(data_dir, aspect2idx):\n",
    "    in_file = os.path.join(data_dir, 'sentihood-train.json')\n",
    "    train = parse_sentihood_json(in_file)\n",
    "    in_file = os.path.join(data_dir, 'sentihood-dev.json')\n",
    "    dev = parse_sentihood_json(in_file)\n",
    "    in_file = os.path.join(data_dir, 'sentihood-test.json')\n",
    "    test = parse_sentihood_json(in_file)\n",
    "\n",
    "    train = convert_input(train, aspect2idx)\n",
    "    train_aspect_idx = get_aspect_idx(train, aspect2idx)\n",
    "    train = tokenize(train)\n",
    "    dev = convert_input(dev, aspect2idx)\n",
    "    dev_aspect_idx = get_aspect_idx(dev, aspect2idx)\n",
    "    dev = tokenize(dev)\n",
    "    test = convert_input(test, aspect2idx)\n",
    "    test_aspect_idx = get_aspect_idx(test, aspect2idx)\n",
    "    test = tokenize(test)\n",
    "\n",
    "    return (train, train_aspect_idx), (dev, dev_aspect_idx), (test, test_aspect_idx)\n",
    "\n",
    "\n",
    "# def get_aspect_idx(data, aspect2idx):\n",
    "#     ret = []\n",
    "#     for _, _, _, aspect, _ in data:\n",
    "#         ret.append(aspect2idx[aspect])\n",
    "#     assert len(data) == len(ret)\n",
    "#     return np.array(ret)\n",
    "\n",
    "\n",
    "def parse_sentihood_json(in_file):\n",
    "    with open(in_file) as f:\n",
    "        data = json.load(f)\n",
    "    ret = []\n",
    "    for d in data:\n",
    "        text = d['text']\n",
    "        sent_id = d['id']\n",
    "        opinions = []\n",
    "        targets = set()\n",
    "        for opinion in d['opinions']:\n",
    "            sentiment = opinion['sentiment']\n",
    "            aspect = opinion['aspect']\n",
    "            target_entity = opinion['target_entity']\n",
    "            targets.add(target_entity)\n",
    "            opinions.append((target_entity, aspect, sentiment))\n",
    "        ret.append((sent_id, text, opinions))\n",
    "    return ret\n",
    "\n",
    "\n",
    "def convert_input(data, all_aspects):\n",
    "    ret = []\n",
    "    for sent_id, text, opinions in data:\n",
    "        for target_entity, aspect, sentiment in opinions:\n",
    "            if aspect not in all_aspects:\n",
    "                continue\n",
    "            ret.append((sent_id, text, target_entity, aspect, sentiment))\n",
    "        assert 'LOCATION1' in text\n",
    "        targets = set(['LOCATION1'])\n",
    "        if 'LOCATION2' in text:\n",
    "            targets.add('LOCATION2')\n",
    "        for target in targets:\n",
    "            aspects = set([a for t, a, _ in opinions if t == target])\n",
    "            none_aspects = [a for a in all_aspects if a not in aspects]\n",
    "            for aspect in none_aspects:\n",
    "                ret.append((sent_id, text, target, aspect, 'None'))\n",
    "    return ret\n",
    "\n",
    "\n",
    "def tokenize(data):\n",
    "    ret = []\n",
    "    for sent_id, text, target_entity, aspect, sentiment in data:\n",
    "        new_text = nltk.word_tokenize(text)\n",
    "        new_aspect = aspect.split('-')\n",
    "        ret.append((sent_id, new_text, target_entity, new_aspect, sentiment))\n",
    "    return ret\n",
    "\n",
    "\n",
    "def all_aspects(data, aspect2idx):\n",
    "    aspect_logits = []\n",
    "    logit = np.zeros(12) #taking care of 0 % 12 \n",
    "    for i , (sent_id, text, target, aspect, sentiment) in enumerate(data, start = 1):\n",
    "        if i % 12 == 0:\n",
    "            aspect_logits.append(logit)\n",
    "            logit = np.zeros(12)\n",
    "        if sentiment != \"None\":\n",
    "            logit[aspect2idx[aspect]] = 1\n",
    "    return aspect_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev = parse_sentihood_json(\"sentihood-dev.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "aspect2idx = {\n",
    "    'general': 0,\n",
    "    'price': 1,\n",
    "    'transit-location': 2,\n",
    "    'safety': 3,\n",
    "    'live' : 4,\n",
    "    'quiet' : 5,\n",
    "    'dining' : 6,\n",
    "    'nightlife' : 7,\n",
    "    'touristy' : 8,\n",
    "    'shopping' : 9,\n",
    "    'green-culture' : 10,\n",
    "    'multicultural' : 11,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(322,\n",
       " '( LOCATION1 is the nearest tube station , just about a 4 min walk ) Youre lucky youre going to be studying in London',\n",
       " [])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "converted_dev = convert_input(dev, aspect2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(302,\n",
       " ' LOCATION1 is just a normal area that happens to have an alternative market',\n",
       " 'LOCATION1',\n",
       " 'shopping',\n",
       " 'Positive')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "converted_dev[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_aspect_id = get_aspect_idx(converted_dev, aspect2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.utils\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "# from torch.optim import lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, datasets, models\n",
    "from torchvision.utils import save_image\n",
    "import os\n",
    "\n",
    "from dataloader import SentihoodDataset\n",
    "from model.nn_model import LSTMModel\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "current_dir = os.getcwd()\n",
    "data_dir = os.path.join(current_dir, \"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "def pad_collate(batch):\n",
    "    batch_embedded_text, a, b, c, d = zip(*batch)\n",
    "    lens = [x.shape[0] for x in batch_embedded_text]\n",
    "    \n",
    "    batch_embedded_text = pad_sequence(batch_embedded_text, batch_first=True, padding_value=0)\n",
    "    \n",
    "    return batch_embedded_text, torch.tensor(lens, dtype=torch.float), a, b, c, d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_dataset = SentihoodDataset(\n",
    "        data_dir,\n",
    "        dataset_type='train',\n",
    "        transform=None,\n",
    "        condition_on_number = False\n",
    "    )\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=10, shuffle=True, num_workers=0, collate_fn = pad_collate)\n",
    "\n",
    "model = LSTMModel(768, 100, 0.0, device, 768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_text, lens, target_index, aspect_logit, c_aspect, sentiment_one_hot = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Input and parameter tensors are not at the same device, found input tensor at cuda:0 and parameter tensor at cpu",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-c539b19dd768>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mdevice\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"cuda\"\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"cpu\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mencoded\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membedded_text\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Desktop\\GitHub\\Targeted-Aspect-Sentiment\\model\\nn_model.py\u001b[0m in \u001b[0;36mencode\u001b[1;34m(self, batch_text, text_lens)\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[0minit_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minit_cell\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit_hidden_lstm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m         \u001b[0mencoder_outputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoder_hidden_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_text\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minit_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minit_cell\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext_lens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoder_hidden_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoder_cell_state\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\GitHub\\Targeted-Aspect-Sentiment\\model\\blocks.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, inputs, hidden, cell, input_lengths)\u001b[0m\n\u001b[0;32m     39\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpack_padded_sequence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_lengths\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menforce_sorted\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_first\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m         \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhidden_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcell_state\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m         \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpad_packed_sequence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_first\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding_value\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\ig++\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 532\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    533\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\ig++\\lib\\site-packages\\torch\\nn\\modules\\rnn.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    559\u001b[0m                               self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[0;32m    560\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 561\u001b[1;33m             result = _VF.lstm(input, batch_sizes, hx, self._flat_weights, self.bias,\n\u001b[0m\u001b[0;32m    562\u001b[0m                               self.num_layers, self.dropout, self.training, self.bidirectional)\n\u001b[0;32m    563\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Input and parameter tensors are not at the same device, found input tensor at cuda:0 and parameter tensor at cpu"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "encoded = model.encode(embedded_text.to(device), lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 39, 768])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedded_text.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[31, 39, 17, 23, 4, 22, 16, 22, 30, 32]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_index[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aspect_logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 1.])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 25, 300])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([25, 300])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.squeeze(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = [[1,2,3], [4,5]]\n",
    "l = [[1], [2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
